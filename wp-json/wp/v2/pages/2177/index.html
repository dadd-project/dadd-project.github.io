{"id":2177,"date":"2021-03-09T10:15:06","date_gmt":"2021-03-09T10:15:06","guid":{"rendered":"http:\/\/dadd-project.org\/?page_id=2177"},"modified":"2021-03-09T11:18:08","modified_gmt":"2021-03-09T11:18:08","slug":"research-streams","status":"publish","type":"page","link":"https:\/\/dadd-project.org\/research-streams\/","title":{"rendered":"Research Streams"},"content":{"rendered":"\n<div class=\"wp-block-group\"><div class=\"wp-block-group__inner-container\"><\/div><\/div>\n\n\n\n<div class=\"wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<h2 class=\"has-large-font-size\"><span style=\"color:#ba0c49\" class=\"has-inline-color\"><strong>1.<\/strong><\/span> Cross-disciplinary understanding of Discrimination<\/h2>\n\n\n\n<p>Addressing and attesting digital discrimination and remedying its corresponding deficiencies is a problem that must be faced from a cross-disciplinary perspective; including the technical, legal and social dimensions of the problem. In this stream, we study the relationship between these dimensions and how these can be combined to better understand discrimination.<\/p>\n\n\n\n<ul><li>Natalia Criado, Jose M. Such.&nbsp;<a href=\"https:\/\/nms.kcl.ac.uk\/jose.such\/pubs\/Digital-Discrimination-AV.pdf\">Digital Discrimination<\/a>. Algorithmic Regulation. Oxford University Press (2019)<\/li><li>Tom van Nuenen, Xavier Ferrer, Jose M. Such, Mark Cot\u00e9. <a rel=\"noreferrer noopener\" href=\"https:\/\/kclpure.kcl.ac.uk\/portal\/files\/130104570\/Transparency_Computer_Magazine.pdf\" target=\"_blank\">Transparency for whom? Assessing discriminatory<\/a><a rel=\"noreferrer noopener\" href=\"https:\/\/www.researchgate.net\/publication\/342082930_Transparency_for_whom_Assessing_discriminatory_AI\" target=\"_blank\"> AI.<\/a> Computer, vol. 53, no. 11, pp. 36\u201344, 2020.<\/li><li>Xavier Ferrer, Tom van Nuenen, Jose M. Such, Mark Cot\u00e9, Natalia Criado. <a rel=\"noreferrer noopener\" href=\"https:\/\/kclpure.kcl.ac.uk\/portal\/files\/139874830\/IEEE_survey_short_bias_and_discrimination_in_AI_copy_camera_ready_Tech_Society_2_.pdf\" target=\"_blank\">Bias and Discrimination in AI: a cross-disciplinary perspective<\/a>. IEEE Technology and Society Magazine (2020) (in press).<\/li><\/ul>\n\n\n\n<hr class=\"wp-block-separator\"\/>\n<\/div><\/div>\n\n\n\n<div style=\"height:100px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<div class=\"wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<h2 class=\"has-large-font-size\"><span style=\"color:#ba0c49\" class=\"has-inline-color\"><strong>2.<\/strong><\/span> Data-driven discovery of biases in ML-based NLP<\/h2>\n\n\n\n<p>Language carries implicit human biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. ML-based NLP methods such as word embeddings have been shown to learn such language biases with striking accuracy. This capability of word embeddings has been successfully exploited as a tool to quantify and study human biases. Here we create a data-driven approach to automatically discover and help interpret conceptual biases towards different concepts encoded in the language from online communities.<\/p>\n\n\n\n<ul><li>Xavier Ferrer, Jose M. Such, Natalia Criado.\u00a0<a href=\"https:\/\/nms.kcl.ac.uk\/jose.such\/pubs\/attesting-bias-language-RAIA.pdf\">Attesting Biases and Discrimination using Language Semantics<\/a>. AAMAS Responsible Artificial Intelligence Agents. (2019)\u00a0<\/li><li>Xavier Ferrer, Tom van Nuenen, Jose M. Such, Natalia Criado. <a href=\"https:\/\/www.researchgate.net\/publication\/343601644_Discovering_and_Categorising_Language_Biases_in_Reddit\">Discovering and Categorising Language Biases in Reddit<\/a>. International AAAI Conference on Web and Social Media (ICWSM 2021) (in press).  <a href=\"https:\/\/github.com\/xfold\/LanguageBiasesInReddit\"><strong><em>Github<\/em><\/strong><\/a><\/li><li>Xavier Ferrer, Tom van Nuenen, Jose M. Such, Natalia Criado. <a href=\"https:\/\/arxiv.org\/pdf\/2010.14448.pdf\">Discovering and Interpreting Conceptual Biases in Online Communities<\/a>. Arxiv preprint (2020).<\/li><\/ul>\n<\/div><\/div>\n\n\n\n<hr class=\"wp-block-separator\"\/>\n\n\n\n<div style=\"height:100px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<div class=\"wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<h2 class=\"has-large-font-size\"><span style=\"color:#ba0c49\" class=\"has-inline-color\"><strong>3.<\/strong><\/span> Study of discrimination in online social media<\/h2>\n\n\n\n<p>Using the Data-driven discovery of biases we created, we explore the biases in social media and online communities and present them in a visually pleasing and interactive website.<\/p>\n\n\n\n<ul><li><a href=\"https:\/\/xfold.github.io\/Web-DiscoveringAndInterpretingConceptualBiases\/\">Discovering And Attesting Conceptual Biases<\/a> Visualiser<\/li><li><a href=\"https:\/\/xfold.github.io\/WE-GenderBiasVisualisationWeb\/\">Language Bias Visualiser<\/a><\/li><\/ul>\n\n\n\n<hr class=\"wp-block-separator\"\/>\n\n\n\n<div style=\"height:100px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<p><\/p>\n<\/div><\/div>\n\n\n\n<h2 class=\"has-large-font-size\"><span style=\"color:#ba0c49\" class=\"has-inline-color\"><strong>4.<\/strong><\/span> Automated assessment of discrimination based on norms<\/h2>\n\n\n\n<p>Biases and discrimination in models and datasets pose a significant challenge to the adoption of ML by companies or public sector organisations, despite ML having the potential to lead to significant reductions in cost and more efficient decisions. Here, we use norms as an abstraction to represent different situations that may lead to digital discrimination to allow non-technical users to benefit from ML. In particular, we formalise non-discrimination norms in the context of ML systems and propose an algorithm to check whether ML systems violate these norms.<\/p>\n\n\n\n<div class=\"wp-block-group\"><div class=\"wp-block-group__inner-container\">\n<ul><li>Natalia Criado, Xavier Ferrer, Jose M. Such. <a rel=\"noreferrer noopener\" href=\"https:\/\/www.researchgate.net\/publication\/343601714_A_Normative_approach_to_Attest_Digital_Discrimination\" target=\"_blank\">A Normative Approach to Attest Digital Discrimination<\/a>. Advancing Towards the SDGS Artificial Intelligence for a Fair, Just and Equitable World, Workshop of the 24th European Conference on Artificial Intelligence (ECAI 2020). <a href=\"https:\/\/github.com\/xfold\/NormativeApproachToDiscrimination\"><strong><em>Github<\/em><\/strong><\/a><\/li><li>Natalia Criado, Xavier Ferrer, Jose M. Such. <a href=\"https:\/\/kclpure.kcl.ac.uk\/portal\/files\/139874694\/Non_discrimination_Norms_Copy_camera_ready_Tech_Society_.pdf\">Is my program sexist? Using Norms to Attest Digital Discrimination<\/a>. IEEE Technology and Society Magazine (2020) (in press).<\/li><li>Natalia Criado, Xavier Ferrer, Jose M. Such. <a href=\"https:\/\/kclpure.kcl.ac.uk\/portal\/files\/141570221\/IJIMAI_copy_for_pure_2_.pdf\">Attesting Digital Discrimination Using Norms.<\/a> International Journal of Interactive Multimedia and Artificial Intelligence IJIMAI (2021) (in press).<\/li><\/ul>\n\n\n\n<hr class=\"wp-block-separator\"\/>\n\n\n\n<div style=\"height:100px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n<\/div><\/div>\n","protected":false},"excerpt":{"rendered":"<p>4. Automated assessment of discrimination based on norms Biases and discrimination in models and datasets pose a significant challenge to the adoption of ML by companies or public sector organisations, despite ML having the potential to lead to significant reductions in cost and more efficient decisions. Here, we use norms as an abstraction to represent &hellip; <\/p>\n<p class=\"link-more\"><a href=\"https:\/\/dadd-project.org\/research-streams\/\" class=\"more-link\">Continue reading<span class=\"screen-reader-text\"> &#8220;Research Streams&#8221;<\/span><\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"","meta":[],"_links":{"self":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/pages\/2177"}],"collection":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/comments?post=2177"}],"version-history":[{"count":13,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/pages\/2177\/revisions"}],"predecessor-version":[{"id":2198,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/pages\/2177\/revisions\/2198"}],"wp:attachment":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/media?parent=2177"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}