{"id":2095,"date":"2020-03-14T17:31:01","date_gmt":"2020-03-14T17:31:01","guid":{"rendered":"http:\/\/dadd-project.org\/?p=2095"},"modified":"2020-03-21T07:54:20","modified_gmt":"2020-03-21T07:54:20","slug":"dadd-language-bias-visualiser","status":"publish","type":"post","link":"https:\/\/dadd-project.org\/2020\/03\/14\/dadd-language-bias-visualiser\/","title":{"rendered":"DADD Language Bias Visualiser"},"content":{"rendered":"\n<p>The DADD Language Bias Visualiser is online! The team has used Word Embeddings to connect target concepts such as `male&#8217; or `female&#8217;  to evaluative attributes found in online data, which are then categorised through clustering algorithms and labelled through a semantic analysis system into more general (conceptual) biases. Categorising biases allows us to give a broad picture of the biases present in discourse communities, such as those on Reddit.<\/p>\n\n\n\n<p>Check it out at <a href=\"https:\/\/xfold.github.io\/WE-GenderBiasVisualisationWeb\/\">https:\/\/xfold.github.io\/WE-GenderBiasVisualisationWeb\/<\/a><\/p>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"1024\" height=\"375\" src=\"https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.21-1024x375.png\" alt=\"\" class=\"wp-image-2096\" srcset=\"https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.21-1024x375.png 1024w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.21-300x110.png 300w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.21-768x281.png 768w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.21-1536x562.png 1536w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.21-2048x750.png 2048w\" sizes=\"(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px\" \/><\/figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"1024\" height=\"358\" src=\"https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.13-1024x358.png\" alt=\"\" class=\"wp-image-2098\" srcset=\"https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.13-1024x358.png 1024w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.13-300x105.png 300w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.13-768x269.png 768w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.13-1536x537.png 1536w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.28.13-2048x716.png 2048w\" sizes=\"(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px\" \/><\/figure>\n\n\n\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" width=\"1024\" height=\"507\" src=\"https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.27.57-1024x507.png\" alt=\"\" class=\"wp-image-2100\" srcset=\"https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.27.57-1024x507.png 1024w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.27.57-300x149.png 300w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.27.57-768x380.png 768w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.27.57-1536x761.png 1536w, https:\/\/dadd-project.org\/wp-content\/uploads\/2020\/03\/Screenshot-2020-03-14-at-19.27.57-2048x1014.png 2048w\" sizes=\"(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px\" \/><\/figure>\n\n\n\n<p><\/p>\n","protected":false},"excerpt":{"rendered":"<p>The DADD Language Bias Visualiser is online! The team has used Word Embeddings to connect target concepts such as `male&#8217; or `female&#8217; to evaluative attributes found in online data, which are then categorised through clustering algorithms and labelled through a semantic analysis system into more general (conceptual) biases. Categorising biases allows us to give a &hellip; <\/p>\n<p class=\"link-more\"><a href=\"https:\/\/dadd-project.org\/2020\/03\/14\/dadd-language-bias-visualiser\/\" class=\"more-link\">Continue reading<span class=\"screen-reader-text\"> &#8220;DADD Language Bias Visualiser&#8221;<\/span><\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"comment_status":"closed","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[1],"tags":[],"_links":{"self":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/posts\/2095"}],"collection":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/comments?post=2095"}],"version-history":[{"count":2,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/posts\/2095\/revisions"}],"predecessor-version":[{"id":2102,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/posts\/2095\/revisions\/2102"}],"wp:attachment":[{"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/media?parent=2095"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/categories?post=2095"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/dadd-project.org\/wp-json\/wp\/v2\/tags?post=2095"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}