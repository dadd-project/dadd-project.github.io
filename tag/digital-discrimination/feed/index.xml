<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>digital discrimination</title>
	<atom:link href="./index.html" rel="self" type="application/rss+xml" />
	<link>./../../../index.html</link>
	<description></description>
	<lastBuildDate>Tue, 06 Oct 2020 18:21:47 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.1</generator>

<image>
	<url>./../../../wp-content/uploads/2020/04/cropped-ICON-2-32x32.png</url>
	<title>digital discrimination</title>
	<link>./../../../index.html</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>SOCINFO2020 TUTORIAL &#8211; DISCOVERING GENDER BIAS AND DISCRIMINATION IN LANGUAGE</title>
		<link>./../../../2020/10/06/socinfo2020-tutorial-discovering-gender-bias-and-discrimination-in-language/index.html</link>
		
		<dc:creator><![CDATA[tomvannuenen]]></dc:creator>
		<pubDate>Tue, 06 Oct 2020 18:16:23 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[digital discrimination]]></category>
		<category><![CDATA[tutorial]]></category>
		<guid isPermaLink="false">./../../../index.html?p=2155</guid>

					<description><![CDATA[Today we presented our tutorial &#8216;DISCOVERING GENDER BIAS AND DISCRIMINATION IN LANGUAGE&#8217; at the online Socinfo2020 conference. It was a great experience for all of us, hope you all enjoyed it! The live session was recorded by the organizers and will probably be shared online very soon. The tutorial focuses on the issue of digital &#8230; <p class="link-more"><a href="./../../../2020/10/06/socinfo2020-tutorial-discovering-gender-bias-and-discrimination-in-language/index.html" class="more-link">Continue reading<span class="screen-reader-text"> "SOCINFO2020 TUTORIAL &#8211; DISCOVERING GENDER BIAS AND DISCRIMINATION IN LANGUAGE"</span></a></p>]]></description>
										<content:encoded><![CDATA[
<p>Today we presented our tutorial &#8216;DISCOVERING GENDER BIAS AND DISCRIMINATION IN LANGUAGE&#8217; at the online Socinfo2020 conference. It was a great experience for all of us, hope you all enjoyed it! The live session was recorded by the organizers and will probably be shared online very soon. </p>



<p>The tutorial focuses on the issue of digital discrimination, particularly towards gender. Its main goal is to help participants improve their digital literacy by understanding the social issues at stake in digital (gender) discrimination, and learning about technical applications and solutions. The tutorial is divided in four parts: it basically iterates twice through the social and technical dimensions. We make use of our own research in language modelling and Word Embeddings in order to clarify how human gender biases may be incorporated into AI/ML models. We first offer a short introduction to digital discrimination and (gender) bias. We give examples of gender discrimination in the field of AI/ML, and discuss the clear gender binary (M/F) that is presupposed when dealing with a computational bias towards gender. We then move to a technical perspective, introducing the DADD Language Bias Visualiser which allows us to discover and analyze gender bias using Word Embeddings. Finally, we show how computational models of bias and discrimination are built on implicit binaries, and discuss with participants the difficulties pertaining to these assumptions in times of post-binary gender attribution. </p>



<p class="has-text-align-center"><img loading="lazy" width="976" height="693" class="wp-image-2160" style="width: 800px;" src="./../../../wp-content/uploads/2020/10/Screenshot-from-2020-10-06-19-20-31.png" alt="" srcset="./../../../wp-content/uploads/2020/10/Screenshot-from-2020-10-06-19-20-31.png 976w, ./../../../wp-content/uploads/2020/10/Screenshot-from-2020-10-06-19-20-31-300x213.png 300w, ./../../../wp-content/uploads/2020/10/Screenshot-from-2020-10-06-19-20-31-768x545.png 768w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px" /></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Discovering and Categorising Language Biases in Reddit</title>
		<link>./../../../2020/08/10/discovering-and-categorising-language-biases-in-reddit/index.html</link>
		
		<dc:creator><![CDATA[tomvannuenen]]></dc:creator>
		<pubDate>Mon, 10 Aug 2020 09:31:18 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[bias]]></category>
		<category><![CDATA[digital discrimination]]></category>
		<category><![CDATA[embeddings]]></category>
		<category><![CDATA[nlp]]></category>
		<category><![CDATA[reddit]]></category>
		<guid isPermaLink="false">./../../../index.html?p=2143</guid>

					<description><![CDATA[Our article &#8220;Discovering and Categorising Language Biases in Reddit&#8221; was accepted last week at the International Conference on Web and Social Media 2021 (ICWSM 2021). Although the proceedings will not be ready until early 2021, you can find the author&#8217;s version of the paper here. We present a method to explore language bias in various &#8230; <p class="link-more"><a href="./../../../2020/08/10/discovering-and-categorising-language-biases-in-reddit/index.html" class="more-link">Continue reading<span class="screen-reader-text"> "Discovering and Categorising Language Biases in Reddit"</span></a></p>]]></description>
										<content:encoded><![CDATA[
<p>Our article &#8220;<a href="https://www.researchgate.net/publication/343498904_Discovering_and_Categorising_Language_Biases_in_Reddit">Discovering and Categorising Language Biases in Reddit</a>&#8221; was accepted last week at the International Conference on Web and Social Media 2021 (ICWSM 2021). Although the proceedings will not be ready until early 2021, you can find the author&#8217;s version of the paper <a href="https://www.researchgate.net/publication/343498904_Discovering_and_Categorising_Language_Biases_in_Reddit">here</a>. </p>



<p>We present a method to explore language bias in various Reddit communities by comparing the words most closely correlated with different concepts leveraging the embedding space of each community. In this way, we study gender bias in r/TheRedPill, religion bias in r/Atheism, and ethnicity bias in r/The_Donald, and discover important biases that, for instance in r/TheRedPill, picture women with words related to externality and physical appearance such as <em>flirtatious</em> and <em>fuckable</em>, and men through descriptive adjectives serving as indicators of subjectivity such as <em>visionary</em> and <em>tactician</em> (see our <a href="https://xfold.github.io/WE-GenderBiasVisualisationWeb/">Bias Visualisation Tool</a>).</p>



<p>Also, in case you are interested in analysing language biases in your own datasets, we are also sharing the code <a href="https://github.com/xfold/LanguageBiasesInReddit">here</a>. Please use it and feel free to ask any questions or report any errors/problems you find on Github!</p>



<p><strong>Abstract.</strong> <em>We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.</em></p>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>A Normative Approach to Attest Digital Discrimination</title>
		<link>./../../../2020/08/03/a-normative-approach-to-attest-digital-discrimination/index.html</link>
		
		<dc:creator><![CDATA[tomvannuenen]]></dc:creator>
		<pubDate>Mon, 03 Aug 2020 09:16:21 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[digital discrimination]]></category>
		<category><![CDATA[github]]></category>
		<guid isPermaLink="false">./../../../index.html?p=2141</guid>

					<description><![CDATA[Our new article &#8220;A Normative Approach to Attest Digital Discrimination&#8221; has been accepted in the Advancing Towards the SDGS Artificial Intelligence for a Fair, Just and Equitable World Workshop (AI4EQ) of the 24th European Conference on Artificial Intelligence (ECAI&#8217;20)! In the paper we formalise non-discrimination norms in the context of ML systems and propose an &#8230; <p class="link-more"><a href="./../../../2020/08/03/a-normative-approach-to-attest-digital-discrimination/index.html" class="more-link">Continue reading<span class="screen-reader-text"> "A Normative Approach to Attest Digital Discrimination"</span></a></p>]]></description>
										<content:encoded><![CDATA[
<p>Our new article &#8220;<a href="https://www.researchgate.net/publication/342944510_A_Normative_approach_to_Attest_Digital_Discrimination">A Normative Approach to Attest Digital Discrimination</a>&#8221; has been accepted in the Advancing Towards the SDGS Artificial Intelligence for a Fair, Just and Equitable World Workshop (AI4EQ) of the 24th European Conference on Artificial Intelligence (ECAI&#8217;20)!</p>



<p>In the paper we formalise non-discrimination norms in the context of ML systems and propose an algorithm to check whether ML systems violate these norms. The code is publicly available <a href="https://github.com/xfold/NormativeApproachToDiscrimination">here</a>. </p>



<p><strong>Abstract.</strong> <em>Digital discrimination is a form of discrimination whereby users are automatically treated unfairly, unethically or just differently based on their personal data by a machine learning (ML) system. Examples of digital discrimination include low-income neighbourhood&#8217;s targeted with high-interest loans or low credit scores, and women being undervalued by 21% in online marketing. Recently, different techniques and tools have been proposed to detect biases that may lead to digital discrimination. These tools often require technical expertise to be executed and for their results to be interpreted. To allow non-technical users to benefit from ML, simpler notions and concepts to represent and reason about digital discrimination are needed. In this paper, we use norms as an abstraction to represent different situations that may lead to digital discrimination. In particular, we formalise non-discrimination norms in the context of ML systems and propose an algorithm to check whether ML systems violate these norms.</em></p>



<p></p>



<p><br><br></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
